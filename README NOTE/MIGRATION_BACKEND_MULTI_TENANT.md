# üîê Guide de Migration Backend - Multi-Tenant Strict

## Vue d'ensemble

Ce document explique comment adapter le code backend Python pour supporter le mode **Multi-Tenant Strict** apr√®s l'ex√©cution de la migration SQL.

---

## üìã Changements Requis

### **1. Mise √† jour de la fonction `find_or_create_cluster`**

üìç Fichier : `/backend/app/workers/tasks.py`

#### **Avant (Single-Tenant)**

```python
def find_or_create_cluster(
    note_id: str,
    pillar_id: str,
    embedding: List[float],
    clarified_content: str
) -> str:
    # Recherche de notes similaires
    similar_notes = supabase.rpc(
        "find_similar_notes",
        {
            "query_embedding": embedding,
            "target_pillar_id": pillar_id,
            "similarity_threshold": 0.75,
            "max_results": 10
        }
    ).execute()
```

#### **Apr√®s (Multi-Tenant Strict)**

```python
def find_or_create_cluster(
    note_id: str,
    pillar_id: str,
    embedding: List[float],
    clarified_content: str,
    organization_id: str  # üîí NOUVEAU PARAM√àTRE
) -> str:
    # Recherche de notes similaires DANS LA M√äME ORGANISATION
    similar_notes = supabase.rpc(
        "find_similar_notes",
        {
            "query_embedding": embedding,
            "target_pillar_id": pillar_id,
            "p_organization_id": organization_id,  # üîí S√âCURIT√â CRITIQUE
            "similarity_threshold": 0.75,
            "max_results": 10
        }
    ).execute()
    
    # ... reste du code inchang√©
    
    # Cr√©er nouveau cluster AVEC organization_id
    cluster_response = supabase.table("clusters").insert({
        "pillar_id": pillar_id,
        "organization_id": organization_id,  # üîí NOUVEAU
        "title": clarified_content[:200] + "...",
        "note_count": 0
    }).execute()
    
    return cluster_response.data[0]["id"]
```

---

### **2. Mise √† jour de `process_note_task`**

üìç Fichier : `/backend/app/workers/tasks.py`

#### **Changements requis**

```python
@celery_app.task(name="process_note", bind=True, max_retries=3)
def process_note_task(self, note_id: str):
    try:
        logger.info(f"Processing note: {note_id}")
        
        # ============================================
        # STEP 1: Fetch Note + User + Organization
        # ============================================
        note_response = supabase.table("notes").select(
            "*, users!inner(job_title, department, seniority_level, organization_id)"  # üîí AJOUT organization_id
        ).eq("id", note_id).single().execute()
        
        if not note_response.data:
            raise ValueError(f"Note {note_id} not found")
        
        note = note_response.data
        user = note["users"]
        organization_id = note["organization_id"]  # üîí R√âCUP√âRATION
        
        # ... (code inchang√© jusqu'√† STEP 2)
        
        # ============================================
        # STEP 2: Get Available Pillars (FILTERED BY ORG)
        # ============================================
        pillars_response = supabase.table("pillars").select("*").eq(
            "organization_id", organization_id  # üîí FILTRAGE PAR ORG
        ).execute()
        available_pillars = pillars_response.data
        
        # ... (STEP 3 et 4 inchang√©s)
        
        # ============================================
        # STEP 5: Find Similar Notes & Cluster (WITH ORG)
        # ============================================
        cluster_id = find_or_create_cluster(
            note_id=note_id,
            pillar_id=pillar["id"],
            embedding=embedding,
            clarified_content=analysis["clarified_content"],
            organization_id=organization_id  # üîí PASSAGE DU PARAM√àTRE
        )
        
        # ... (reste inchang√©)
        
    except Exception as e:
        logger.error(f"‚ùå Error processing note {note_id}: {e}")
        # ... (gestion d'erreur inchang√©e)
```

---

### **3. Mise √† jour de `generate_cluster_snapshot_task`**

üìç Fichier : `/backend/app/workers/tasks.py`

#### **Changements requis**

```python
@celery_app.task(name="generate_cluster_snapshot")
def generate_cluster_snapshot_task(cluster_id: str):
    try:
        logger.info(f"Generating snapshot for cluster: {cluster_id}")
        
        # ============================================
        # STEP 1: Fetch Cluster + Notes (WITH ORG CHECK)
        # ============================================
        cluster_response = supabase.table("clusters").select(
            "*, pillars(name), notes!inner(*, users(department, job_title))"
        ).eq("id", cluster_id).eq(
            "notes.status", "processed"
        ).single().execute()
        
        if not cluster_response.data:
            raise ValueError(f"Cluster {cluster_id} not found")
        
        cluster = cluster_response.data
        notes = cluster["notes"]
        organization_id = cluster["organization_id"]  # üîí R√âCUP√âRATION
        
        # ... (code de g√©n√©ration inchang√©)
        
        # ============================================
        # STEP 6: Create Snapshot (WITH ORG)
        # ============================================
        supabase.table("cluster_snapshots").insert({
            "cluster_id": cluster_id,
            "organization_id": organization_id,  # üîí NOUVEAU
            "synthesis_text": synthesis,
            "metrics_json": metrics,
            "included_note_ids": [n["id"] for n in notes],
            "note_count": len(notes),
            "avg_relevance_score": avg_score
        }).execute()
        
        # ... (reste inchang√©)
        
    except Exception as e:
        logger.error(f"‚ùå Error generating snapshot for cluster {cluster_id}: {e}")
        raise
```

---

### **4. Mise √† jour des routes API**

üìç Fichier : `/backend/app/api/routes/notes.py`

#### **4.1 Route `POST /notes`**

```python
@router.post("/", response_model=NoteResponse)
async def create_note(note: NoteCreate, organization_id: str = Header(..., alias="X-Organization-ID")):
    """
    Create a single note (online mode)
    Returns immediately with "draft" status
    """
    try:
        # Insert note WITH organization_id
        response = supabase.table("notes").insert({
            "user_id": str(note.user_id),
            "content_raw": note.content_raw,
            "organization_id": organization_id,  # üîí NOUVEAU
            "status": "draft"
        }).execute()
        
        created_note = response.data[0]
        note_id = created_note["id"]
        
        # Trigger async processing
        process_note_task.delay(note_id)
        
        logger.info(f"Note created: {note_id} (org: {organization_id})")
        
        return created_note
        
    except Exception as e:
        logger.error(f"Error creating note: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

#### **4.2 Route `GET /notes/user/{user_id}`**

```python
@router.get("/notes/user/{user_id}")
async def get_user_notes(user_id: str, organization_id: str = Header(..., alias="X-Organization-ID")):
    """
    Get all notes for a specific user (for Track Queue page)
    FILTERED BY ORGANIZATION
    """
    try:
        # Query notes filtered by user_id AND organization_id
        query = supabase.table("notes").select(
            """
            id,
            content_raw,
            content_clarified,
            status,
            created_at,
            processed_at,
            ai_relevance_score,
            cluster_id,
            clusters(id, title, pillar_id, note_count, pillars(id, name))
            """
        ).eq("user_id", user_id).eq(
            "organization_id", organization_id  # üîí FILTRAGE PAR ORG
        )
            
        response = query.order("created_at", desc=True).execute()
        
        # ... (reste du code inchang√©)
        
    except Exception as e:
        logger.error(f"‚ùå Error fetching user notes: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

#### **4.3 Autres routes**

Appliquer le m√™me pattern √† toutes les routes :
- `GET /notes/{note_id}` : V√©rifier que `note.organization_id == organization_id`
- `PATCH /notes/{note_id}` : V√©rifier que `note.organization_id == organization_id`
- `DELETE /notes/{note_id}` : V√©rifier que `note.organization_id == organization_id`
- `GET /notes/{note_id}/timeline` : Filtrer par `organization_id`

---

### **5. Middleware pour extraire `organization_id`**

üìç Fichier : `/backend/app/middleware/organization.py` (NOUVEAU)

```python
"""
Middleware to extract and validate organization_id from request
"""
from fastapi import Request, HTTPException
from starlette.middleware.base import BaseHTTPMiddleware
from loguru import logger

class OrganizationMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # Extract organization_id from header
        org_id = request.headers.get("X-Organization-ID")
        
        # Skip for public routes
        if request.url.path in ["/health", "/docs", "/openapi.json"]:
            return await call_next(request)
        
        # Validate organization_id
        if not org_id:
            logger.error("Missing X-Organization-ID header")
            raise HTTPException(
                status_code=400,
                detail="Missing X-Organization-ID header"
            )
        
        # Validate UUID format
        try:
            from uuid import UUID
            UUID(org_id)
        except ValueError:
            logger.error(f"Invalid organization_id format: {org_id}")
            raise HTTPException(
                status_code=400,
                detail="Invalid organization_id format"
            )
        
        # Store in request state
        request.state.organization_id = org_id
        
        # Continue processing
        response = await call_next(request)
        return response
```

#### **Enregistrement du middleware**

üìç Fichier : `/backend/main.py`

```python
from app.middleware.organization import OrganizationMiddleware

app = FastAPI(title="SIGMENT API")

# Add organization middleware
app.add_middleware(OrganizationMiddleware)
```

---

### **6. Mise √† jour des mod√®les Pydantic**

üìç Fichier : `/backend/app/models/note.py`

```python
class NoteCreate(BaseModel):
    """Note creation payload from frontend"""
    content_raw: str = Field(..., min_length=10, max_length=5000)
    user_id: UUID
    # organization_id sera extrait du header, pas du body

class NoteResponse(BaseModel):
    """Note response model"""
    id: UUID
    user_id: UUID
    organization_id: UUID  # üîí NOUVEAU
    content_raw: str
    content_clarified: Optional[str] = None
    pillar_id: Optional[UUID] = None
    cluster_id: Optional[UUID] = None
    ai_relevance_score: Optional[float] = None
    status: str
    created_at: datetime
    processed_at: Optional[datetime] = None
```

---

## üß™ Tests de Validation

### **Test 1 : Isolation des notes**

```python
# Test que les notes d'une org ne sont pas visibles par une autre org
def test_note_isolation():
    # Cr√©er une note dans org A
    note_org_a = create_note(user_id="user1", org_id="org-a", content="Test A")
    
    # Essayer de r√©cup√©rer depuis org B
    response = get_note(note_id=note_org_a["id"], org_id="org-b")
    
    # Doit retourner 404 ou 403
    assert response.status_code in [403, 404]
```

### **Test 2 : Clustering isol√©**

```python
# Test que le clustering ne m√©lange pas les orgs
def test_cluster_isolation():
    # Cr√©er 2 notes similaires dans org A
    note1_org_a = create_note(org_id="org-a", content="Carbon footprint tracking")
    note2_org_a = create_note(org_id="org-a", content="Carbon emissions monitoring")
    
    # Cr√©er 1 note similaire dans org B
    note1_org_b = create_note(org_id="org-b", content="Carbon footprint system")
    
    # Attendre le traitement
    time.sleep(10)
    
    # V√©rifier que note1_org_a et note2_org_a sont dans le m√™me cluster
    note1_data = get_note(note1_org_a["id"], org_id="org-a")
    note2_data = get_note(note2_org_a["id"], org_id="org-a")
    assert note1_data["cluster_id"] == note2_data["cluster_id"]
    
    # V√©rifier que note1_org_b est dans un cluster diff√©rent
    note_b_data = get_note(note1_org_b["id"], org_id="org-b")
    assert note_b_data["cluster_id"] != note1_data["cluster_id"]
```

### **Test 3 : Pillars par organisation**

```python
# Test que chaque org a ses propres pillars
def test_pillars_isolation():
    # R√©cup√©rer les pillars de org A
    pillars_org_a = get_pillars(org_id="org-a")
    
    # R√©cup√©rer les pillars de org B
    pillars_org_b = get_pillars(org_id="org-b")
    
    # V√©rifier qu'ils sont diff√©rents
    pillar_ids_a = {p["id"] for p in pillars_org_a}
    pillar_ids_b = {p["id"] for p in pillars_org_b}
    assert pillar_ids_a.isdisjoint(pillar_ids_b)
```

---

## üöÄ D√©ploiement

### **√âtape 1 : Ex√©cuter la migration SQL**

```bash
# Se connecter √† la base de donn√©es
psql -h <host> -U <user> -d <database>

# Ex√©cuter la migration
\i database/migrate_notes_multi_tenant_strict.sql

# V√©rifier les r√©sultats
SELECT table_name, column_name, is_nullable 
FROM information_schema.columns 
WHERE column_name = 'organization_id' 
AND table_name IN ('pillars', 'notes', 'clusters', 'note_events', 'cluster_snapshots');
```

### **√âtape 2 : Mettre √† jour le code backend**

```bash
# Appliquer tous les changements list√©s ci-dessus
# Tester localement avec pytest
pytest tests/test_multi_tenant.py -v

# V√©rifier les logs Celery
celery -A app.workers.celery_app worker --loglevel=debug
```

### **√âtape 3 : Mettre √† jour le frontend**

```typescript
// Ajouter le header X-Organization-ID √† toutes les requ√™tes
const api = {
    baseURL: process.env.NEXT_PUBLIC_API_URL,
    headers: {
        'Content-Type': 'application/json',
        'X-Organization-ID': getCurrentOrganizationId() // üîí NOUVEAU
    }
};
```

---

## ‚ö†Ô∏è Points d'Attention

### **1. Migration des donn√©es existantes**

Si vous avez d√©j√† des donn√©es en production :

```sql
-- Assigner toutes les donn√©es existantes √† une org par d√©faut
UPDATE notes SET organization_id = (SELECT id FROM organizations WHERE slug = 'default-org') WHERE organization_id IS NULL;
UPDATE clusters SET organization_id = (SELECT id FROM organizations WHERE slug = 'default-org') WHERE organization_id IS NULL;
UPDATE pillars SET organization_id = (SELECT id FROM organizations WHERE slug = 'default-org') WHERE organization_id IS NULL;
```

### **2. Performance**

Les index composites cr√©√©s garantissent de bonnes performances :
- `idx_notes_org_status` : Pour les requ√™tes de statut
- `idx_notes_org_user` : Pour les requ√™tes utilisateur
- `idx_clusters_org_pillar` : Pour les requ√™tes de clustering

### **3. S√©curit√©**

- ‚úÖ Row Level Security (RLS) activ√©
- ‚úÖ Contraintes de validation cross-org
- ‚úÖ Fonction `find_similar_notes` s√©curis√©e
- ‚úÖ Middleware de validation

---

## üìö Checklist de Migration

- [ ] Ex√©cuter `migrate_notes_multi_tenant_strict.sql`
- [ ] Mettre √† jour `find_or_create_cluster` avec `organization_id`
- [ ] Mettre √† jour `process_note_task` pour r√©cup√©rer `organization_id`
- [ ] Mettre √† jour `generate_cluster_snapshot_task` avec `organization_id`
- [ ] Ajouter header `X-Organization-ID` √† toutes les routes API
- [ ] Cr√©er `OrganizationMiddleware`
- [ ] Mettre √† jour les mod√®les Pydantic
- [ ] Mettre √† jour le frontend pour envoyer `X-Organization-ID`
- [ ] √âcrire et ex√©cuter les tests d'isolation
- [ ] V√©rifier les logs Celery
- [ ] D√©ployer en production

---

**Derni√®re mise √† jour :** 2 d√©cembre 2025  
**Version :** 1.0.0
